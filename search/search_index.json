{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"OG-Learn","text":"<p>Overfit-to-Generalization Framework for Spatiotemporal Machine Learning</p> <p> </p>"},{"location":"#what-is-og-learn","title":"What is OG-Learn?","text":"<p>OG-Learn implements the Overfit-to-Generalization (OG) framework, a two-stage approach designed to improve the generalization capability of machine learning models in spatiotemporal prediction tasks.</p>"},{"location":"#the-problem","title":"The Problem","text":"<p>Traditional machine learning models often struggle with spatiotemporal data due to:</p> <ul> <li>Local overfitting: Models may fit well to dense regions but fail in sparse areas</li> <li>Spatial heterogeneity: Different regions exhibit different patterns</li> <li>Temporal non-stationarity: Patterns evolve over time</li> </ul>"},{"location":"#the-og-solution","title":"The OG Solution","text":"<p>The OG framework addresses these challenges through a two-stage process:</p> <pre><code>flowchart LR\n    A[Data] --&gt; B[Stage 1: HV Model]\n    B --&gt; C[Pseudo-labels]\n    C --&gt; D[Stage 2: LV Model]\n    D --&gt; E[Final Prediction]\n\n    style B fill:#e1bee7\n    style D fill:#bbdefb</code></pre> <ol> <li>Stage 1 - High-Variance (HV) Model: Use a flexible model (e.g., LightGBM) to generate pseudo-labels with density-aware sampling and oscillation noise</li> <li>Stage 2 - Low-Variance (LV) Model: Train a regularized model (e.g., MLP, ResNet) to learn from pseudo-labels, capturing generalizable patterns</li> </ol>"},{"location":"#quick-example","title":"Quick Example","text":"<pre><code>from og_learn import OGModel, compare_models\n\n# Create an OG model with preset configurations\nmodel = OGModel(\n    hv='lightgbm',      # High-variance model\n    lv='mlp',           # Low-variance model  \n    oscillation=0.05,   # Feature noise for regularization\n    sampling_alpha=0.1  # Density-aware sampling weight\n)\n\n# Train the model\nmodel.fit(X_train, y_train, density=density_train, epochs=100)\n\n# Make predictions\npredictions = model.predict(X_test)\n</code></pre>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li> <p>:material-lightning-bolt:{ .lg .middle } Easy to Use</p> <p>Simple scikit-learn-like API with <code>fit()</code> and <code>predict()</code> methods</p> </li> <li> <p>:material-cog:{ .lg .middle } Flexible Presets</p> <p>Built-in presets for common HV models (LightGBM, XGBoost, CatBoost) and LV models (MLP, ResNet, Transformer)</p> </li> <li> <p>:material-tune:{ .lg .middle } Fully Customizable</p> <p>Use any model with standard <code>fit()</code>/<code>predict()</code> interface as HV or LV component</p> </li> <li> <p>:material-chart-line:{ .lg .middle } TensorBoard Integration</p> <p>Real-time training visualization with built-in TensorBoard support</p> </li> </ul>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install og-learn\n</code></pre> <p>Or install from source:</p> <pre><code>git clone https://github.com/your-username/og-learn.git\ncd og-learn\npip install -e .\n</code></pre>"},{"location":"#citation","title":"Citation","text":"<p>If you use OG-Learn in your research, please cite:</p> <pre><code>@article{oglearn2025,\n  title={Overfit-to-Generalization: A Framework for Spatiotemporal Machine Learning},\n  author={Your Name},\n  journal={Journal Name},\n  year={2024}\n}\n</code></pre>"},{"location":"#license","title":"License","text":"<p>OG-Learn is released under the MIT License.</p>"},{"location":"about/contributing/","title":"Contributing","text":"<p>We welcome contributions to OG-Learn!</p>"},{"location":"about/contributing/#development-setup","title":"Development Setup","text":"<ol> <li>Clone the repository:</li> </ol> <pre><code>git clone https://github.com/your-username/og-learn.git\ncd og-learn\n</code></pre> <ol> <li>Create a virtual environment:</li> </ol> <pre><code>python -m venv venv\nsource venv/bin/activate  # Linux/Mac\nvenv\\Scripts\\activate     # Windows\n</code></pre> <ol> <li>Install in development mode:</li> </ol> <pre><code>pip install -e \".[dev]\"\n</code></pre>"},{"location":"about/contributing/#code-style","title":"Code Style","text":"<ul> <li>Follow PEP 8</li> <li>Use type hints where practical</li> <li>Write docstrings for public functions</li> </ul>"},{"location":"about/contributing/#running-tests","title":"Running Tests","text":"<pre><code>pytest tests/\n</code></pre>"},{"location":"about/contributing/#documentation","title":"Documentation","text":"<p>Build docs locally:</p> <pre><code>mkdocs serve\n</code></pre> <p>View at http://localhost:8000</p>"},{"location":"about/contributing/#pull-requests","title":"Pull Requests","text":"<ol> <li>Fork the repository</li> <li>Create a feature branch</li> <li>Make your changes</li> <li>Run tests</li> <li>Submit a pull request</li> </ol>"},{"location":"about/contributing/#issues","title":"Issues","text":"<p>Report bugs and request features via GitHub Issues.</p>"},{"location":"about/license/","title":"License","text":"<p>OG-Learn is released under the MIT License.</p> <pre><code>MIT License\n\nCopyright (c) 2024 Your Name\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n</code></pre>"},{"location":"api/feature/","title":"Feature Engineering API","text":"<p>Utilities for spatiotemporal feature engineering.</p>"},{"location":"api/feature/#simple_feature_engineering","title":"simple_feature_engineering","text":"<pre><code>from og_learn.feature import simple_feature_engineering\n\ndf_processed, feature_cols = simple_feature_engineering(\n    df,\n    feature_cols,\n    time_col='time',\n    k=3,\n    standardize=True,\n    add_temporal=True\n)\n</code></pre> <p>Apply complete feature engineering pipeline.</p> <p>Parameters:</p> Parameter Type Default Description <code>df</code> DataFrame - Input data <code>feature_cols</code> list - Original feature column names <code>time_col</code> str <code>'time'</code> Name of time column <code>k</code> int <code>3</code> Number of spatial harmonics <code>standardize</code> bool <code>True</code> Apply StandardScaler <code>add_temporal</code> bool <code>True</code> Add temporal features <p>Returns:  - <code>df_processed</code> (DataFrame): Processed data - <code>feature_cols</code> (list): Updated feature column names</p>"},{"location":"api/feature/#compute_spatial_harmonics","title":"compute_spatial_harmonics","text":"<pre><code>from og_learn.feature import compute_spatial_harmonics\n\ndf = compute_spatial_harmonics(df, k=3, lon_col='longitude', lat_col='latitude')\n</code></pre> <p>Add spatial harmonic features.</p> <p>Parameters:</p> Parameter Type Default Description <code>df</code> DataFrame - Input data <code>k</code> int <code>3</code> Number of harmonics <code>lon_col</code> str <code>'longitude'</code> Longitude column name <code>lat_col</code> str <code>'latitude'</code> Latitude column name <p>Returns: DataFrame with added harmonic columns</p> <p>Created columns: - <code>lon_sin_1</code>, <code>lon_cos_1</code>, ..., <code>lon_sin_k</code>, <code>lon_cos_k</code> - <code>lat_sin_1</code>, <code>lat_cos_1</code>, ..., <code>lat_sin_k</code>, <code>lat_cos_k</code></p>"},{"location":"api/feature/#compute_temporal_features","title":"compute_temporal_features","text":"<pre><code>from og_learn.feature import compute_temporal_features\n\ndf = compute_temporal_features(df, time_col='time')\n</code></pre> <p>Add temporal features from datetime column.</p> <p>Parameters:</p> Parameter Type Default Description <code>df</code> DataFrame - Input data <code>time_col</code> str <code>'time'</code> Time column name (datetime) <p>Returns: DataFrame with added temporal columns</p> <p>Created columns: - <code>time_month</code>: Month (1-12), normalized to [0, 1] - <code>time_hour</code>: Hour (0-23), normalized to [0, 1] - <code>time_day_of_month</code>: Day (1-31), normalized to [0, 1]</p>"},{"location":"api/feature/#example","title":"Example","text":"<pre><code>import pandas as pd\nfrom og_learn.feature import (\n    compute_spatial_harmonics,\n    compute_temporal_features,\n    simple_feature_engineering\n)\n\n# Load data\ndf = pd.read_parquet('data.parquet')\n\n# Option 1: Step-by-step\ndf = compute_temporal_features(df, time_col='time')\ndf = compute_spatial_harmonics(df, k=3)\n\n# Option 2: Complete pipeline\ndf, feature_cols = simple_feature_engineering(\n    df,\n    feature_cols=['longitude', 'latitude', 'temp', 'pressure'],\n    k=3,\n    standardize=True\n)\n</code></pre>"},{"location":"api/ogmodel/","title":"OGModel","text":"<p>The main class for the Overfit-to-Generalization framework.</p>"},{"location":"api/ogmodel/#class-definition","title":"Class Definition","text":""},{"location":"api/ogmodel/#og_learn.framework.OGModel","title":"<code>og_learn.framework.OGModel</code>","text":"<p>               Bases: <code>BaseEstimator</code>, <code>RegressorMixin</code></p> <p>Overfit-to-Generalization Model</p> <p>Combines a high-variance (HV) model for pseudo-label generation with a  low-variance (LV) model for generalization, using density-aware sampling.</p> <p>The OG framework works by: 1. Training HV model (e.g., LightGBM) on original data 2. Each epoch of LV training:    - Sample data with density-aware weighting (prioritize sparse regions)    - Add noise to features (oscillation)    - Generate pseudo-labels from HV model    - Train LV model on pseudo-labels</p>"},{"location":"api/ogmodel/#og_learn.framework.OGModel--parameters","title":"Parameters","text":"<p>hv : str or object     High-variance model. Can be:     - str: 'lightgbm', 'xgboost', 'catboost' (uses preset config)     - object: Custom model with fit/predict interface</p> str or object <p>Low-variance model. Can be: - str: 'mlp', 'resnet', 'transformer' (uses preset config) - object: Custom model with fit/predict interface</p> float, default=0.05 <p>Noise injection level for pseudo-labels (controls regularization)</p> float, default=0.1 <p>Weight for density-aware sampling (0=uniform, higher=more sparse-focused)</p> int, default=100 <p>Number of training epochs for LV model</p> bool, default=True <p>Whether to use early stopping</p> int, default=5 <p>Early stopping patience (epochs without improvement)</p> bool, default=True <p>Whether to print training progress</p>"},{"location":"api/ogmodel/#og_learn.framework.OGModel--examples","title":"Examples","text":""},{"location":"api/ogmodel/#og_learn.framework.OGModel--preset-og","title":"Preset OG","text":"<p>og = OGModel(hv='lightgbm', lv='mlp') og.fit(X_train, y_train, density=density_values) predictions = og.predict(X_test)</p>"},{"location":"api/ogmodel/#og_learn.framework.OGModel--custom-og","title":"Custom OG","text":"<p>from lightgbm import LGBMRegressor custom_hv = LGBMRegressor(n_estimators=500, num_leaves=300) og = OGModel(hv=custom_hv, lv='mlp', oscillation=0.03) og.fit(X_train, y_train, density=density_values)</p>"},{"location":"api/ogmodel/#og_learn.framework.OGModel.__init__","title":"<code>__init__(hv='lightgbm', lv='mlp', oscillation=0.05, sampling_alpha=0.1, epochs=100, early_stopping=False, patience=5, eval_every_epochs=5, verbose=True, seed=42, tensorboard_dir=None, tensorboard_name=None)</code>","text":""},{"location":"api/ogmodel/#og_learn.framework.OGModel.fit","title":"<code>fit(X, y, density=None, X_valid=None, y_valid=None)</code>","text":"<p>Fit the OG model.</p>"},{"location":"api/ogmodel/#og_learn.framework.OGModel.fit--parameters","title":"Parameters","text":"<p>X : array-like of shape (n_samples, n_features)     Training features</p> array-like of shape (n_samples,) <p>Training targets</p> array-like of shape (n_samples,), optional <p>Data density at each sample location. Higher values = denser areas. If None, uniform sampling is used (no density-aware weighting).</p> array-like, optional <p>Validation features for early stopping</p> array-like, optional <p>Validation targets for early stopping</p>"},{"location":"api/ogmodel/#og_learn.framework.OGModel.fit--returns","title":"Returns","text":"<p>self</p>"},{"location":"api/ogmodel/#og_learn.framework.OGModel.predict","title":"<code>predict(X)</code>","text":"<p>Predict using the fitted LV model.</p>"},{"location":"api/ogmodel/#og_learn.framework.OGModel.predict--parameters","title":"Parameters","text":"<p>X : array-like of shape (n_samples, n_features)     Features to predict</p>"},{"location":"api/ogmodel/#og_learn.framework.OGModel.predict--returns","title":"Returns","text":"<p>y_pred : ndarray of shape (n_samples,)     Predicted values</p>"},{"location":"api/ogmodel/#constructor","title":"Constructor","text":"<pre><code>OGModel(\n    hv='lightgbm',\n    lv='mlp',\n    oscillation=0.05,\n    sampling_alpha=0.1,\n    epochs=100,\n    early_stopping=True,\n    patience=10,\n    seed=42,\n    verbose=True,\n    tensorboard_dir=None,\n    tensorboard_name=None,\n    eval_every_epochs=10\n)\n</code></pre>"},{"location":"api/ogmodel/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>hv</code> str or model <code>'lightgbm'</code> High-variance model. Can be a preset name or a model instance with <code>fit()</code>/<code>predict()</code> methods <code>lv</code> str or model <code>'mlp'</code> Low-variance model. Can be a preset name or a model instance <code>oscillation</code> float <code>0.05</code> Noise injection strength for pseudo-label generation <code>sampling_alpha</code> float <code>0.1</code> Exponent for density-aware sampling weights <code>epochs</code> int <code>100</code> Number of training epochs for LV model <code>early_stopping</code> bool <code>True</code> Whether to use early stopping <code>patience</code> int <code>10</code> Early stopping patience (epochs without improvement) <code>seed</code> int <code>42</code> Random seed for reproducibility <code>verbose</code> bool <code>True</code> Whether to print training progress <code>tensorboard_dir</code> str <code>None</code> Directory for TensorBoard logs <code>tensorboard_name</code> str <code>None</code> Name for this run in TensorBoard <code>eval_every_epochs</code> int <code>10</code> Frequency of evaluation/logging"},{"location":"api/ogmodel/#methods","title":"Methods","text":""},{"location":"api/ogmodel/#fit","title":"fit","text":"<pre><code>model.fit(X, y, density=None, X_valid=None, y_valid=None, epochs=None)\n</code></pre> <p>Train the OG model.</p> <p>Parameters:</p> Parameter Type Description <code>X</code> array-like Training features, shape (n_samples, n_features) <code>y</code> array-like Training target, shape (n_samples,) <code>density</code> array-like Spatial density for each sample, shape (n_samples,) <code>X_valid</code> array-like Validation features (optional) <code>y_valid</code> array-like Validation target (optional) <code>epochs</code> int Override epochs from constructor <p>Returns: <code>self</code></p> <p>Example:</p> <pre><code>model = OGModel(hv='lightgbm', lv='mlp')\nmodel.fit(\n    X_train, y_train,\n    density=density_train,\n    X_valid=X_valid,\n    y_valid=y_valid,\n    epochs=100\n)\n</code></pre>"},{"location":"api/ogmodel/#predict","title":"predict","text":"<pre><code>predictions = model.predict(X)\n</code></pre> <p>Make predictions.</p> <p>Parameters:</p> Parameter Type Description <code>X</code> array-like Features, shape (n_samples, n_features) <p>Returns: <code>numpy.ndarray</code> - Predictions, shape (n_samples,)</p> <p>Example:</p> <pre><code>predictions = model.predict(X_test)\n</code></pre>"},{"location":"api/ogmodel/#attributes","title":"Attributes","text":"Attribute Type Description <code>_hv_model</code> object Fitted HV model instance <code>_lv_model</code> object Fitted LV model instance <code>hv_name</code> str Name of HV model <code>lv_name</code> str Name of LV model"},{"location":"api/ogmodel/#complete-example","title":"Complete Example","text":"<pre><code>from og_learn import OGModel, calculate_density\nfrom sklearn.metrics import r2_score\nimport numpy as np\n\n# Prepare data\ndensity = calculate_density(X_train[:, 0], X_train[:, 1])\n\n# Create and train model\nmodel = OGModel(\n    hv='lightgbm',\n    lv='resnet',\n    oscillation=0.05,\n    sampling_alpha=0.1,\n    epochs=100,\n    early_stopping=True,\n    patience=15,\n    seed=42,\n    tensorboard_dir='runs/og_resnet'\n)\n\nmodel.fit(\n    X_train, y_train,\n    density=density,\n    X_valid=X_valid,\n    y_valid=y_valid\n)\n\n# Evaluate\npredictions = model.predict(X_test)\nprint(f\"Test R\u00b2: {r2_score(y_test, predictions):.4f}\")\n</code></pre>"},{"location":"api/presets/","title":"Presets API","text":"<p>Functions for accessing preset model configurations.</p>"},{"location":"api/presets/#list_presets","title":"list_presets","text":"<pre><code>from og_learn import list_presets\n\nlist_presets()\n</code></pre> <p>Print all available HV and LV presets.</p>"},{"location":"api/presets/#get_hv_model","title":"get_hv_model","text":"<pre><code>from og_learn.presets import get_hv_model\n\nmodel = get_hv_model(name)\n</code></pre> <p>Get a High-Variance model instance.</p> <p>Parameters:</p> Parameter Type Description <code>name</code> str Preset name: <code>'lightgbm'</code>, <code>'biglightgbm'</code>, <code>'xgboost'</code>, <code>'catboost'</code>, <code>'random_forest'</code>, <code>'decision_tree'</code>, <code>'linear_regression'</code> <p>Returns: Model instance with <code>fit()</code>/<code>predict()</code> methods</p> <p>Example:</p> <pre><code>from og_learn.presets import get_hv_model\n\nlgb = get_hv_model('lightgbm')\nlgb.fit(X_train, y_train)\npredictions = lgb.predict(X_test)\n</code></pre>"},{"location":"api/presets/#get_lv_model","title":"get_lv_model","text":"<pre><code>from og_learn.presets import get_lv_model\n\nmodel = get_lv_model(name, num_features, epochs=100)\n</code></pre> <p>Get a Low-Variance model instance.</p> <p>Parameters:</p> Parameter Type Description <code>name</code> str Preset name: <code>'mlp'</code>, <code>'bigmlp'</code>, <code>'resnet'</code>, <code>'transformer'</code> <code>num_features</code> int Number of input features <code>epochs</code> int Training epochs (default: 100) <p>Returns: Model instance with <code>fit()</code>/<code>predict()</code> methods</p> <p>Example:</p> <pre><code>from og_learn.presets import get_lv_model\n\nmlp = get_lv_model('mlp', num_features=10, epochs=50)\nmlp.fit(X_train, y_train)\npredictions = mlp.predict(X_test)\n</code></pre>"},{"location":"api/presets/#preset-configurations","title":"Preset Configurations","text":""},{"location":"api/presets/#hv_presets","title":"HV_PRESETS","text":"<p>Dictionary of HV model configurations:</p> <pre><code>HV_PRESETS = {\n    'lightgbm': {\n        'n_estimators': 500,\n        'num_leaves': 1200,\n        'max_depth': 9,\n        'learning_rate': 0.05,\n        # ...\n    },\n    'biglightgbm': { ... },\n    'xgboost': { ... },\n    'catboost': { ... },\n    'random_forest': { ... },\n    'decision_tree': { ... },\n    'linear_regression': { ... },\n}\n</code></pre>"},{"location":"api/presets/#lv_presets","title":"LV_PRESETS","text":"<p>Dictionary of LV model configurations:</p> <pre><code>LV_PRESETS = {\n    'mlp': {\n        'hidden_layers': [256, 128, 64],\n        'dropout': 0.3,\n        'batch_size': 256,\n        'learning_rate': 0.001,\n    },\n    'bigmlp': { ... },\n    'resnet': { ... },\n    'transformer': { ... },\n}\n</code></pre>"},{"location":"api/utils/","title":"Utilities API","text":"<p>Helper functions for data loading, validation, and more.</p>"},{"location":"api/utils/#load_data","title":"load_data","text":"<pre><code>from og_learn import load_data\n\ndf = load_data(data_path, feature_cols, target_col)\n</code></pre> <p>Load and preprocess data from file.</p> <p>Parameters:</p> Parameter Type Description <code>data_path</code> str Path to data file (CSV, Parquet, etc.) <code>feature_cols</code> list Expected feature column names <code>target_col</code> str Target column name <p>Returns: <code>pandas.DataFrame</code></p> <p>Operations performed: - Converts <code>time</code> column to datetime - Converts float64 to float32 - Filters to valid feature columns</p>"},{"location":"api/utils/#sanity_check","title":"sanity_check","text":"<pre><code>from og_learn import sanity_check\n\nsanity_check(df, feature_cols, target_col)\n</code></pre> <p>Validate data and print summary.</p> <p>Parameters:</p> Parameter Type Description <code>df</code> DataFrame Data to validate <code>feature_cols</code> list Feature column names <code>target_col</code> str Target column name"},{"location":"api/utils/#calculate_density","title":"calculate_density","text":"<pre><code>from og_learn import calculate_density\n\ndensity = calculate_density(longitude, latitude, method='kde')\n</code></pre> <p>Calculate spatial density for each point.</p> <p>Parameters:</p> Parameter Type Default Description <code>longitude</code> array - Longitude values <code>latitude</code> array - Latitude values <code>method</code> str <code>'kde'</code> Density estimation method <p>Returns: <code>numpy.ndarray</code> - Density values</p>"},{"location":"api/utils/#split_test_train","title":"split_test_train","text":"<pre><code>from og_learn import split_test_train\n\ntrain_idx, test_idx = split_test_train(df, method='site', test_ratio=0.2)\n</code></pre> <p>Split data into train/test sets.</p> <p>Parameters:</p> Parameter Type Default Description <code>df</code> DataFrame - Data with site information <code>method</code> str <code>'site'</code> Split method: <code>'site'</code>, <code>'random'</code>, <code>'temporal'</code> <code>test_ratio</code> float <code>0.2</code> Fraction for test set <p>Returns:  - <code>train_idx</code>: Training indices - <code>test_idx</code>: Test indices</p>"},{"location":"api/utils/#save_split_indices-load_split_indices","title":"save_split_indices / load_split_indices","text":"<pre><code>from og_learn import save_split_indices, load_split_indices\n\n# Save indices\nsave_split_indices(train_idx, test_idx, save_dir='data/splits')\n\n# Load indices\ntrain_idx, test_idx = load_split_indices(save_dir='data/splits')\n</code></pre> <p>Persist train/test splits for reproducibility.</p>"},{"location":"api/utils/#launch_tensorboard","title":"launch_tensorboard","text":"<pre><code>from og_learn import launch_tensorboard\n\ntb_process = launch_tensorboard(log_dir, port=6006, open_browser=True)\n</code></pre> <p>Start TensorBoard server.</p> <p>Parameters:</p> Parameter Type Default Description <code>log_dir</code> str - Directory containing TensorBoard logs <code>port</code> int <code>6006</code> Port for TensorBoard server <code>open_browser</code> bool <code>True</code> Open browser automatically <p>Returns: <code>subprocess.Popen</code> - TensorBoard process handle</p> <p>Stop TensorBoard:</p> <pre><code>tb_process.terminate()\n</code></pre>"},{"location":"api/utils/#compare_models","title":"compare_models","text":"<pre><code>from og_learn import compare_models\n\nresults = compare_models(\n    models,\n    X_train, y_train,\n    X_test, y_test,\n    density=None,\n    tensorboard_dir=None,\n    save_dir=None,\n    eval_every_epochs=10\n)\n</code></pre> <p>Train and compare multiple models.</p> <p>Parameters:</p> Parameter Type Description <code>models</code> dict Dictionary of <code>X_train</code> array Training features <code>y_train</code> array Training target <code>X_test</code> array Test features <code>y_test</code> array Test target <code>density</code> array Density for OG models (optional) <code>tensorboard_dir</code> str TensorBoard log directory (optional) <code>save_dir</code> str Directory to save trained models (optional) <code>eval_every_epochs</code> int Logging frequency <p>Returns: <code>pandas.DataFrame</code> with columns <code>['Model', 'Train_R2', 'Test_R2']</code></p>"},{"location":"examples/OG_Tutorial/","title":"OG-Learn Tutorial","text":"In\u00a0[1]: Copied! <pre>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import r2_score\nimport sys\nfrom pathlib import Path\n%load_ext autoreload\n%autoreload 2\n\n# Add og_learn to path (for development)\nOG_LEARN_ROOT = Path(\"..\").resolve()\nif str(OG_LEARN_ROOT) not in sys.path:\n    sys.path.insert(0, str(OG_LEARN_ROOT))\n    \nfrom og_learn import (\n    OGModel, compare_models, \n    calculate_density, split_test_train, simple_feature_engineering,\n    load_data, sanity_check, save_split_indices\n)\n\nprint(\"og-learn imported successfully!\")\n</pre> import numpy as np import pandas as pd import matplotlib.pyplot as plt from sklearn.metrics import r2_score import sys from pathlib import Path %load_ext autoreload %autoreload 2  # Add og_learn to path (for development) OG_LEARN_ROOT = Path(\"..\").resolve() if str(OG_LEARN_ROOT) not in sys.path:     sys.path.insert(0, str(OG_LEARN_ROOT))      from og_learn import (     OGModel, compare_models,      calculate_density, split_test_train, simple_feature_engineering,     load_data, sanity_check, save_split_indices )  print(\"og-learn imported successfully!\")  <pre>og-learn imported successfully!\n</pre> In\u00a0[2]: Copied! <pre># ============================================================\n# CONFIGURATION - Modify for your dataset\n# ============================================================\nDATA_PATH = 'data/df_example.pkl'  # Replace with your data\nSAVE_DIR = 'diagnostics'           # Directory to save indices and diagnostics\nSAMPLE = 100000                      # Sample size (None for full dataset)\n\nTARGET_COL = 'Ozone'  # Target column\n\nFEATURE_COLS = [\n    'TROPOMI_ozone', 'OMI_ozone', 'tco3', 'blh', 't2m', 'sp', 'WS10',\n    'population', 'no2', 'DSR', 'strd', 'r_1000', 'lai_hv', 'pev',\n    'ssro', 't_975', 't_925', 'tp', 'tsn', 'stl1', 'time', 'longitude', 'latitude'\n]\n\n# ============================================================\n# Load Data (using utility function)\n# ============================================================\ndf, FEATURE_COLS = load_data(DATA_PATH, FEATURE_COLS, TARGET_COL)\n\n# Sanity check\nsanity_check(df, FEATURE_COLS, TARGET_COL)\n</pre> # ============================================================ # CONFIGURATION - Modify for your dataset # ============================================================ DATA_PATH = 'data/df_example.pkl'  # Replace with your data SAVE_DIR = 'diagnostics'           # Directory to save indices and diagnostics SAMPLE = 100000                      # Sample size (None for full dataset)  TARGET_COL = 'Ozone'  # Target column  FEATURE_COLS = [     'TROPOMI_ozone', 'OMI_ozone', 'tco3', 'blh', 't2m', 'sp', 'WS10',     'population', 'no2', 'DSR', 'strd', 'r_1000', 'lai_hv', 'pev',     'ssro', 't_975', 't_925', 'tp', 'tsn', 'stl1', 'time', 'longitude', 'latitude' ]  # ============================================================ # Load Data (using utility function) # ============================================================ df, FEATURE_COLS = load_data(DATA_PATH, FEATURE_COLS, TARGET_COL)  # Sanity check sanity_check(df, FEATURE_COLS, TARGET_COL)  <pre>\u2713 Loaded 3,520,452 samples from 1,302 locations\n  Time range: 2019-06-01 00:00:00 to 2019-09-29 23:00:00\n  Target: Ozone\n  Features: 23 columns\n\u2713 Sanity check passed\n\nFeature engineering options:\n  \u2713 Temporal harmonics available (time column found)\n  \u2713 Spatial harmonics available (longitude/latitude found)\n</pre> Out[2]: <pre>True</pre> In\u00a0[3]: Copied! <pre># ============================================================\n# Split Data (Site-wise)\n# ============================================================\n# Site-wise split ensures no spatial leakage between train/test\n_, _, train_idx, test_idx, df = split_test_train(\n    df, sample=SAMPLE, split=0.2, flag='Site', seed=42, verbose=1\n)\n\n# Save split indices to diagnostics folder\nsave_split_indices(train_idx, test_idx, save_dir=SAVE_DIR)\n\n# ============================================================\n# Calculate Density using TRAIN stations only\n# ============================================================\n# This ensures test density is calculated based on train network\n# (simulates real-world scenario where test locations are \"new\")\ndf = calculate_density(df, radius=500, reference_idx=train_idx)\n\nprint(f\"\\n\u2713 Density calculated (based on train stations)\")\nprint(f\"  Train density: {df.loc[train_idx, 'density'].min():.2e} - {df.loc[train_idx, 'density'].max():.2e}\")\nprint(f\"  Test density:  {df.loc[test_idx, 'density'].min():.2e} - {df.loc[test_idx, 'density'].max():.2e}\")\n</pre> # ============================================================ # Split Data (Site-wise) # ============================================================ # Site-wise split ensures no spatial leakage between train/test _, _, train_idx, test_idx, df = split_test_train(     df, sample=SAMPLE, split=0.2, flag='Site', seed=42, verbose=1 )  # Save split indices to diagnostics folder save_split_indices(train_idx, test_idx, save_dir=SAVE_DIR)  # ============================================================ # Calculate Density using TRAIN stations only # ============================================================ # This ensures test density is calculated based on train network # (simulates real-world scenario where test locations are \"new\") df = calculate_density(df, radius=500, reference_idx=train_idx)  print(f\"\\n\u2713 Density calculated (based on train stations)\") print(f\"  Train density: {df.loc[train_idx, 'density'].min():.2e} - {df.loc[train_idx, 'density'].max():.2e}\") print(f\"  Test density:  {df.loc[test_idx, 'density'].min():.2e} - {df.loc[test_idx, 'density'].max():.2e}\")  <pre>Processing sample size: 100000\nUsing flag: Site\nSelected Site Count: 260, (19.97%)\nSelected DataRow Count: 19765, (19.77%)\nTraining Site Count: 1042, (80.03%)\nTraining DataRow Count: 80235, (80.23%)\n\u2713 Split indices saved to diagnostics/\n  - split_train_idx.npy (80,235 samples)\n  - split_test_idx.npy (19,765 samples)\n</pre> <pre>Calculating density (r=500km):   0%|          | 0/1302 [00:00&lt;?, ?it/s]</pre> <pre>\n\u2713 Density calculated (based on train stations)\n  Train density: 1.81e-06 - 1.71e-04\n  Test density:  2.76e-06 - 1.70e-04\n</pre> In\u00a0[4]: Copied! <pre># ============================================================\n# Feature Engineering\n# ============================================================\n# Temporal (add_temporal_harmonics=True):\n#   - Extracts: time_month, time_day_of_month, time_hour\n#   - Computes: T1, T2, T3 (temporal harmonics)\n#   - Drops: 'time' column\n# Spatial (add_spatial_harmonics=True):\n#   - Computes: S1, S2, S3 (using original lon/lat)\n#   - Drops: 'longitude', 'latitude'\n\nX, y, FINAL_FEATURES = simple_feature_engineering(\n    df,\n    feature_cols=FEATURE_COLS,\n    target_col=TARGET_COL,\n    add_spatial_harmonics=True,   # S1, S2, S3\n    add_temporal_harmonics=True,  # T1, T2, T3, time_month, time_day_of_month, time_hour\n    standardize=True\n)\n\n# Split into train/test\nX_train = X.loc[train_idx].values\nX_test = X.loc[test_idx].values\ny_train = y.loc[train_idx].values\ny_test = y.loc[test_idx].values\n\n# Density for OG training (training samples only)\ndensity_train = df.loc[train_idx, 'density'].values\n\nprint(f\"\\n\u2713 Features prepared: {X_train.shape}\")\nprint(f\"\u2713 Final features: {FINAL_FEATURES[:5]}... ({len(FINAL_FEATURES)} total)\")\n</pre> # ============================================================ # Feature Engineering # ============================================================ # Temporal (add_temporal_harmonics=True): #   - Extracts: time_month, time_day_of_month, time_hour #   - Computes: T1, T2, T3 (temporal harmonics) #   - Drops: 'time' column # Spatial (add_spatial_harmonics=True): #   - Computes: S1, S2, S3 (using original lon/lat) #   - Drops: 'longitude', 'latitude'  X, y, FINAL_FEATURES = simple_feature_engineering(     df,     feature_cols=FEATURE_COLS,     target_col=TARGET_COL,     add_spatial_harmonics=True,   # S1, S2, S3     add_temporal_harmonics=True,  # T1, T2, T3, time_month, time_day_of_month, time_hour     standardize=True )  # Split into train/test X_train = X.loc[train_idx].values X_test = X.loc[test_idx].values y_train = y.loc[train_idx].values y_test = y.loc[test_idx].values  # Density for OG training (training samples only) density_train = df.loc[train_idx, 'density'].values  print(f\"\\n\u2713 Features prepared: {X_train.shape}\") print(f\"\u2713 Final features: {FINAL_FEATURES[:5]}... ({len(FINAL_FEATURES)} total)\")  <pre>Feature Engineering Pipeline: fill NA \u2192 temporal harmonics \u2192 spatial harmonics \u2192 standardize\n  \u2713 Added temporal features: time_month, time_day_of_month, time_hour, T1, T2, T3\n  \u2713 Standardized all features\n  \u2713 Added spatial harmonics: S1, S2, S3 (replaced lon/lat)\n\nFinal features (29): ['TROPOMI_ozone', 'OMI_ozone', 'tco3', 'blh', 't2m']...\n\n\u2713 Features prepared: (80235, 29)\n\u2713 Final features: ['TROPOMI_ozone', 'OMI_ozone', 'tco3', 'blh', 't2m']... (29 total)\n</pre> In\u00a0[5]: Copied! <pre># ============================================================\n# View all available presets\n# ============================================================\nfrom og_learn import list_presets\nlist_presets()\n</pre> # ============================================================ # View all available presets # ============================================================ from og_learn import list_presets list_presets()  <pre>============================================================\nHV Models (High-Variance for pseudo-label generation):\n------------------------------------------------------------\n  - lightgbm\n  - biglightgbm\n  - xgboost\n  - bigxgboost\n  - catboost\n  - bigcatboost\n  - random_forest\n  - decision_tree\n  - linear_regression\n\n============================================================\nLV Models (Low-Variance for generalization):\n------------------------------------------------------------\n  - mlp\n    Also: mlp_early (with early stopping)\n  - bigmlp\n    Also: bigmlp_early (with early stopping)\n  - resnet\n    Also: resnet_early (with early stopping)\n  - transformer\n    Also: transformer_early (with early stopping)\n============================================================\n</pre> In\u00a0[6]: Copied! <pre># # ============================================================\n# # Preset OG: LightGBM (HV) + MLP (LV)\n# # ============================================================\n# og = OGModel(\n#     hv='biglightgbm',       # High-variance: LightGBM for overfitting\n#     lv='bigmlp',            # Low-variance: MLP for generalization\n#     oscillation=0.05,\n#     sampling_alpha=0.1,\n#     epochs=300,\n#     early_stopping=False, # Enable early stopping\n#     # patience=5,          # Stop after 5 epochs without improvement\n#     eval_every_epochs=10  # Evaluate every 5 epochs\n# )\n\n# # Fit with density and validation data\n# og.fit(\n#     X_train, y_train, \n#     density=density_train,\n#     X_valid=X_test,      # Use test as validation for early stopping\n#     y_valid=y_test\n# )\n\n# # Predict and evaluate\n# y_pred = og.predict(X_test)\n# r2 = r2_score(y_test, y_pred)\n# print(f\"\\n\u2713 OG Model Test R\u00b2: {r2:.4f}\")\n</pre> # # ============================================================ # # Preset OG: LightGBM (HV) + MLP (LV) # # ============================================================ # og = OGModel( #     hv='biglightgbm',       # High-variance: LightGBM for overfitting #     lv='bigmlp',            # Low-variance: MLP for generalization #     oscillation=0.05, #     sampling_alpha=0.1, #     epochs=300, #     early_stopping=False, # Enable early stopping #     # patience=5,          # Stop after 5 epochs without improvement #     eval_every_epochs=10  # Evaluate every 5 epochs # )  # # Fit with density and validation data # og.fit( #     X_train, y_train,  #     density=density_train, #     X_valid=X_test,      # Use test as validation for early stopping #     y_valid=y_test # )  # # Predict and evaluate # y_pred = og.predict(X_test) # r2 = r2_score(y_test, y_pred) # print(f\"\\n\u2713 OG Model Test R\u00b2: {r2:.4f}\")  In\u00a0[7]: Copied! <pre># # ============================================================\n# # Custom HV Model\n# # ============================================================\n# from lightgbm import LGBMRegressor\n\n# # Custom LightGBM with specific parameters\n# custom_lgb = LGBMRegressor(\n#     n_estimators=500,\n#     num_leaves=500,\n#     max_depth=25,\n#     learning_rate=0.1,\n#     min_child_samples=10,\n#     verbose=-1\n# )\n\n# # Use custom HV with preset LV\n# og_custom_hv = OGModel(\n#     hv=custom_lgb,  # Your custom model\n#     lv='mlp',\n#     oscillation=0.03  # Less noise\n# )\n\n# og_custom_hv.fit(X_train, y_train, density=density_train)\n# print(f\"Custom HV Test R\u00b2: {og_custom_hv.score(X_test, y_test):.4f}\")\n</pre> # # ============================================================ # # Custom HV Model # # ============================================================ # from lightgbm import LGBMRegressor  # # Custom LightGBM with specific parameters # custom_lgb = LGBMRegressor( #     n_estimators=500, #     num_leaves=500, #     max_depth=25, #     learning_rate=0.1, #     min_child_samples=10, #     verbose=-1 # )  # # Use custom HV with preset LV # og_custom_hv = OGModel( #     hv=custom_lgb,  # Your custom model #     lv='mlp', #     oscillation=0.03  # Less noise # )  # og_custom_hv.fit(X_train, y_train, density=density_train) # print(f\"Custom HV Test R\u00b2: {og_custom_hv.score(X_test, y_test):.4f}\")  In\u00a0[8]: Copied! <pre># # ============================================================\n# # Custom LV Model (Simple MLP example)\n# # ============================================================\n# from sklearn.neural_network import MLPRegressor as SklearnMLP\n\n# # Custom sklearn MLP\n# custom_mlp = SklearnMLP(\n#     hidden_layer_sizes=(256, 128, 64),\n#     activation='relu',\n#     max_iter=200,\n#     early_stopping=True,\n#     validation_fraction=0.1,\n#     random_state=42\n# )\n\n# # Use preset HV with custom LV\n# og_custom_lv = OGModel(\n#     hv='lightgbm',\n#     lv=custom_mlp,  # Your custom model\n#     oscillation=0.05\n# )\n\n# og_custom_lv.fit(X_train, y_train, density=density_train)\n# print(f\"Custom LV Test R\u00b2: {og_custom_lv.score(X_test, y_test):.4f}\")\n</pre> # # ============================================================ # # Custom LV Model (Simple MLP example) # # ============================================================ # from sklearn.neural_network import MLPRegressor as SklearnMLP  # # Custom sklearn MLP # custom_mlp = SklearnMLP( #     hidden_layer_sizes=(256, 128, 64), #     activation='relu', #     max_iter=200, #     early_stopping=True, #     validation_fraction=0.1, #     random_state=42 # )  # # Use preset HV with custom LV # og_custom_lv = OGModel( #     hv='lightgbm', #     lv=custom_mlp,  # Your custom model #     oscillation=0.05 # )  # og_custom_lv.fit(X_train, y_train, density=density_train) # print(f\"Custom LV Test R\u00b2: {og_custom_lv.score(X_test, y_test):.4f}\")  <p>View training curves in browser using TensorBoard. All models share the same dashboard.</p> <p>Usage:</p> <ol> <li>Run cell below to start training with TensorBoard logging</li> <li>Open http://localhost:6006 in browser</li> <li>View real-time training curves for all models</li> </ol> In\u00a0[16]: Copied! <pre># ============================================================\n# TensorBoard: View training curves in browser\n# ============================================================\nimport shutil\nimport os\nfrom og_learn.framework import launch_tensorboard\nfrom og_learn.presets import get_hv_model, get_lv_model\nfrom sklearn.linear_model import Ridge\n\n# Clear old logs for fresh start\nTB_LOG_DIR = 'diagnostics/tensorboard'\n\n# Launch TensorBoard (opens http://localhost:6006)\ntb_process = launch_tensorboard(TB_LOG_DIR, open_browser=True)\n\n# Run comparison with TensorBoard logging\n# Training curves will be logged and viewable in real-time\nEPOCHS = 200\nEVAL_EVERY = 5  # Log to TensorBoard every 5 epochs\n\nmodels_tb = {\n    # === HV Models (all presets) ===\n    'LightGBM': get_hv_model('lightgbm'),\n    'BigLightGBM': get_hv_model('biglightgbm'),\n    'XGBoost': get_hv_model('xgboost'),\n    'BigXGBoost': get_hv_model('bigxgboost'),\n    'CatBoost': get_hv_model('catboost'),\n    'BigCatBoost': get_hv_model('bigcatboost'),\n    'RandomForest': get_hv_model('random_forest'),\n    'DecisionTree': get_hv_model('decision_tree'),\n    \n    # === LV Models (with TensorBoard logging) ===\n    'MLP': get_lv_model('mlp', num_features=X_train.shape[1], epochs=EPOCHS),\n    'BigMLP': get_lv_model('bigmlp', num_features=X_train.shape[1], epochs=EPOCHS),\n    'Transformer': get_lv_model('transformer', num_features=X_train.shape[1], epochs=EPOCHS),\n    'Resnet': get_lv_model('resnet', num_features=X_train.shape[1], epochs=EPOCHS),\n        \n    # === OG Models ===\n    'OG_MLP': OGModel(hv='lightgbm', lv='mlp', epochs=EPOCHS, eval_every_epochs=EVAL_EVERY), \n    'OG_BigMLP': OGModel(hv='biglightgbm', lv='bigmlp', epochs=EPOCHS, eval_every_epochs=EVAL_EVERY),\n    \n    # === OG Models ===\n    'CatBoost_MLP': OGModel(hv='catboost', lv='mlp', epochs=EPOCHS, eval_every_epochs=EVAL_EVERY), \n    'CatBoost_BigMLP': OGModel(hv='catboost', lv='bigmlp', epochs=EPOCHS, eval_every_epochs=EVAL_EVERY),\n\n    # === OG Models ===\n    'CatBoost_RESNET': OGModel(hv='catboost', lv='resnet', epochs=EPOCHS, eval_every_epochs=EVAL_EVERY), \n    'BigCatBoost_RESNET': OGModel(hv='bigcatboost', lv='resnet', epochs=EPOCHS, eval_every_epochs=EVAL_EVERY), \n\n\n    \n}\n\nresults_tb = compare_models(\n    X_train, y_train, X_test, y_test,\n    models=models_tb,\n    density=density_train,\n    tensorboard_dir=TB_LOG_DIR,  # &lt;-- Enable TensorBoard logging\n    eval_every_epochs=EVAL_EVERY,  # Log every 5 epochs\n    save_dir=TB_LOG_DIR  # Save/load models (same dir as tensorboard)\n)\n\n# When done, stop TensorBoard:\n# tb_process.terminate()\n</pre> # ============================================================ # TensorBoard: View training curves in browser # ============================================================ import shutil import os from og_learn.framework import launch_tensorboard from og_learn.presets import get_hv_model, get_lv_model from sklearn.linear_model import Ridge  # Clear old logs for fresh start TB_LOG_DIR = 'diagnostics/tensorboard'  # Launch TensorBoard (opens http://localhost:6006) tb_process = launch_tensorboard(TB_LOG_DIR, open_browser=True)  # Run comparison with TensorBoard logging # Training curves will be logged and viewable in real-time EPOCHS = 200 EVAL_EVERY = 5  # Log to TensorBoard every 5 epochs  models_tb = {     # === HV Models (all presets) ===     'LightGBM': get_hv_model('lightgbm'),     'BigLightGBM': get_hv_model('biglightgbm'),     'XGBoost': get_hv_model('xgboost'),     'BigXGBoost': get_hv_model('bigxgboost'),     'CatBoost': get_hv_model('catboost'),     'BigCatBoost': get_hv_model('bigcatboost'),     'RandomForest': get_hv_model('random_forest'),     'DecisionTree': get_hv_model('decision_tree'),          # === LV Models (with TensorBoard logging) ===     'MLP': get_lv_model('mlp', num_features=X_train.shape[1], epochs=EPOCHS),     'BigMLP': get_lv_model('bigmlp', num_features=X_train.shape[1], epochs=EPOCHS),     'Transformer': get_lv_model('transformer', num_features=X_train.shape[1], epochs=EPOCHS),     'Resnet': get_lv_model('resnet', num_features=X_train.shape[1], epochs=EPOCHS),              # === OG Models ===     'OG_MLP': OGModel(hv='lightgbm', lv='mlp', epochs=EPOCHS, eval_every_epochs=EVAL_EVERY),      'OG_BigMLP': OGModel(hv='biglightgbm', lv='bigmlp', epochs=EPOCHS, eval_every_epochs=EVAL_EVERY),          # === OG Models ===     'CatBoost_MLP': OGModel(hv='catboost', lv='mlp', epochs=EPOCHS, eval_every_epochs=EVAL_EVERY),      'CatBoost_BigMLP': OGModel(hv='catboost', lv='bigmlp', epochs=EPOCHS, eval_every_epochs=EVAL_EVERY),      # === OG Models ===     'CatBoost_RESNET': OGModel(hv='catboost', lv='resnet', epochs=EPOCHS, eval_every_epochs=EVAL_EVERY),      'BigCatBoost_RESNET': OGModel(hv='bigcatboost', lv='resnet', epochs=EPOCHS, eval_every_epochs=EVAL_EVERY),         }  results_tb = compare_models(     X_train, y_train, X_test, y_test,     models=models_tb,     density=density_train,     tensorboard_dir=TB_LOG_DIR,  # &lt;-- Enable TensorBoard logging     eval_every_epochs=EVAL_EVERY,  # Log every 5 epochs     save_dir=TB_LOG_DIR  # Save/load models (same dir as tensorboard) )  # When done, stop TensorBoard: # tb_process.terminate()  <pre>\ud83d\udcca TensorBoard started at http://localhost:6006\n   Log directory: diagnostics/tensorboard\n============================================================\nModel Comparison\n============================================================\n\ud83d\udcca TensorBoard: tensorboard --logdir=diagnostics/tensorboard\n   Open http://localhost:6006 to view training curves\n\nLoading: LightGBM (from diagnostics/tensorboard\\LightGBM\\model.pkl)\n  \u2192 R\u00b2: 0.5653 (loaded)\n\nLoading: BigLightGBM (from diagnostics/tensorboard\\BigLightGBM\\model.pkl)\n  \u2192 R\u00b2: 0.5658 (loaded)\n\nLoading: XGBoost (from diagnostics/tensorboard\\XGBoost\\model.pkl)\n  \u2192 R\u00b2: 0.5663 (loaded)\n\nLoading: BigXGBoost (from diagnostics/tensorboard\\BigXGBoost\\model.pkl)\n  \u2192 R\u00b2: 0.5659 (loaded)\n\nTraining: CatBoost...\n  \u2192 R\u00b2: 0.5755\n  \ud83d\udcbe Saved to diagnostics/tensorboard\\CatBoost\\model.pkl\n\nLoading: BigCatBoost (from diagnostics/tensorboard\\BigCatBoost\\model.pkl)\n  \u2192 R\u00b2: 0.5687 (loaded)\n\nLoading: RandomForest (from diagnostics/tensorboard\\RandomForest\\model.pkl)\n  \u2192 R\u00b2: 0.5228 (loaded)\n\nLoading: DecisionTree (from diagnostics/tensorboard\\DecisionTree\\model.pkl)\n  \u2192 R\u00b2: 0.1684 (loaded)\n\nLoading: MLP (from diagnostics/tensorboard\\MLP\\model.pkl)\n  \u2192 R\u00b2: 0.5541 (loaded)\n\nLoading: BigMLP (from diagnostics/tensorboard\\BigMLP\\model.pkl)\n  \u2192 R\u00b2: 0.4839 (loaded)\n\nLoading: Transformer (from diagnostics/tensorboard\\Transformer\\model.pkl)\n  \u2192 R\u00b2: 0.5189 (loaded)\n\nLoading: Resnet (from diagnostics/tensorboard\\Resnet\\model.pkl)\n</pre> <pre>Predicting:   0%|          | 0/1 [00:00&lt;?, ?it/s]</pre> <pre>  \u2192 R\u00b2: 0.4784 (loaded)\n\nLoading: OG_MLP (from diagnostics/tensorboard\\OG_MLP\\model.pkl)\n  \u2192 R\u00b2: 0.5667 (loaded)\n\nLoading: OG_BigMLP (from diagnostics/tensorboard\\OG_BigMLP\\model.pkl)\n  \u2192 R\u00b2: 0.5793 (loaded)\n\nLoading: CatBoost_MLP (from diagnostics/tensorboard\\CatBoost_MLP\\model.pkl)\n  \u2192 R\u00b2: 0.5813 (loaded)\n\nLoading: CatBoost_BigMLP (from diagnostics/tensorboard\\CatBoost_BigMLP\\model.pkl)\n  \u2192 R\u00b2: 0.5792 (loaded)\n\nLoading: CatBoost_RESNET (from diagnostics/tensorboard\\CatBoost_RESNET\\model.pkl)\n</pre> <pre>Predicting:   0%|          | 0/1 [00:00&lt;?, ?it/s]</pre> <pre>  \u2192 R\u00b2: 0.5852 (loaded)\n\nLoading: BigCatBoost_RESNET (from diagnostics/tensorboard\\BigCatBoost_RESNET\\model.pkl)\n</pre> <pre>Predicting:   0%|          | 0/1 [00:00&lt;?, ?it/s]</pre> <pre>  \u2192 R\u00b2: 0.5813 (loaded)\n\n============================================================\nSummary:\n------------------------------------------------------------\n  CatBoost_RESNET: 0.5852\n  BigCatBoost_RESNET: 0.5813\n  CatBoost_MLP: 0.5813\n  OG_BigMLP: 0.5793\n  CatBoost_BigMLP: 0.5792\n  CatBoost: 0.5755\n  BigCatBoost: 0.5687\n  OG_MLP: 0.5667\n  XGBoost: 0.5663\n  BigXGBoost: 0.5659\n  BigLightGBM: 0.5658\n  LightGBM: 0.5653\n  MLP: 0.5541\n  RandomForest: 0.5228\n  Transformer: 0.5189\n  BigMLP: 0.4839\n  Resnet: 0.4784\n  DecisionTree: 0.1684\n============================================================\n</pre> In\u00a0[17]: Copied! <pre># ============================================================\n# Visualize comparison\n# ============================================================\nfig, ax = plt.subplots(figsize=(10, 5))\n\nnames = list(results_tb.keys())\nscores = [results_tb[n] if results_tb[n] is not None else 0 for n in names]\n\n# Sort by score (descending) and get top 3 model names\nsorted_pairs = sorted(zip(names, scores), key=lambda x: x[1], reverse=True)\ntop3_names = {pair[0] for pair in sorted_pairs[:3]}\n\n# Color top 3 models in green\ncolors = ['#2ca02c' if n in top3_names else '#1f77b4' for n in names]\n\nbars = ax.barh(names, scores, color=colors, alpha=0.8)\nax.set_xlabel('R\u00b2 Score', fontsize=12)\nax.set_title('Model Comparison (Top 3 in Green)', fontsize=14)\nax.grid(axis='x', alpha=0.3)\n\n# Add value labels\nfor bar, score in zip(bars, scores):\n    ax.text(bar.get_width() + 0.01, bar.get_y() + bar.get_height()/2,\n            f'{score:.4f}', va='center', fontsize=10)\n\nplt.tight_layout()\nplt.show()\n</pre> # ============================================================ # Visualize comparison # ============================================================ fig, ax = plt.subplots(figsize=(10, 5))  names = list(results_tb.keys()) scores = [results_tb[n] if results_tb[n] is not None else 0 for n in names]  # Sort by score (descending) and get top 3 model names sorted_pairs = sorted(zip(names, scores), key=lambda x: x[1], reverse=True) top3_names = {pair[0] for pair in sorted_pairs[:3]}  # Color top 3 models in green colors = ['#2ca02c' if n in top3_names else '#1f77b4' for n in names]  bars = ax.barh(names, scores, color=colors, alpha=0.8) ax.set_xlabel('R\u00b2 Score', fontsize=12) ax.set_title('Model Comparison (Top 3 in Green)', fontsize=14) ax.grid(axis='x', alpha=0.3)  # Add value labels for bar, score in zip(bars, scores):     ax.text(bar.get_width() + 0.01, bar.get_y() + bar.get_height()/2,             f'{score:.4f}', va='center', fontsize=10)  plt.tight_layout() plt.show()"},{"location":"examples/OG_Tutorial/#og-learn-tutorial","title":"OG-Learn Tutorial\u00b6","text":"<p>Overfit-to-Generalization Framework for Equitable Spatiotemporal Modeling</p> <p>This notebook demonstrates how to use <code>og-learn</code> to address local overfitting in spatiotemporal models.</p>"},{"location":"examples/OG_Tutorial/#contents","title":"Contents\u00b6","text":"<ol> <li>Data Preparation - Load data, calculate density, split train/test</li> <li>Quick Start - Preset OG with default parameters</li> <li>Custom OG - Use your own HV/LV models</li> <li>Model Comparison - Compare OG vs standard models</li> <li>Understanding OG - How it works</li> </ol>"},{"location":"examples/OG_Tutorial/#0-data-preparation","title":"0. Data Preparation\u00b6","text":"<p>Load real spatiotemporal data, calculate density, and split into train/test sets.</p>"},{"location":"examples/OG_Tutorial/#configuration","title":"Configuration\u00b6","text":"Variable Description <code>DATA_PATH</code> Path to your pickle file <code>TARGET_COL</code> Column name for target values <code>FEATURE_COLS</code> List of feature column names"},{"location":"examples/OG_Tutorial/#split-data-and-calculate-density","title":"Split Data and Calculate Density\u00b6","text":"<p>(Density can be customized by your own function)</p>"},{"location":"examples/OG_Tutorial/#feature-engineering","title":"Feature Engineering\u00b6","text":""},{"location":"examples/OG_Tutorial/#1-quick-start-preset-og","title":"1. Quick Start - Preset OG\u00b6","text":"<p>Now that data is prepared, use OG with preset models.</p>"},{"location":"examples/OG_Tutorial/#available-presets-from-get_model_configs","title":"Available Presets (from <code>get_model_configs</code>)\u00b6","text":"<p>HV Models (High-Variance for pseudo-label generation):</p> Name Description <code>lightgbm</code> Standard LightGBM (100 trees, 300 leaves) <code>biglightgbm</code> Large LightGBM (1200 leaves) <code>xgboost</code> Standard XGBoost (100 trees) <code>bigxgboost</code> Large XGBoost (1200 trees) <code>catboost</code> Standard CatBoost (300 iters, depth 8) <code>bigcatboost</code> Large CatBoost (1000 iters, depth 7) <code>random_forest</code> RandomForest (5 trees, depth 20) <code>decision_tree</code> DecisionTree (depth 25) <p>LV Models (Low-Variance for generalization):</p> Name Description <code>mlp</code> Standard MLP [512, 256, 128] <code>bigmlp</code> Large MLP [1024, 512\u00d76, 256\u00d75, 128, 64] <code>resnet</code> ResNet (6 blocks, 512 hidden) <code>transformer</code> FT-Transformer (depth 6, dim 32) <code>gtransformer</code> Global FT-Transformer"},{"location":"examples/OG_Tutorial/#2-custom-og-your-own-models","title":"2. Custom OG - Your Own Models\u00b6","text":"<p>Use any model with <code>fit(X, y)</code> and <code>predict(X)</code> interface.</p>"},{"location":"examples/OG_Tutorial/#3-model-comparison","title":"3. Model Comparison\u00b6","text":"<p>Compare OG models against standard baselines.</p>"},{"location":"examples/OG_Tutorial/#31-standard-ml-models","title":"3.1 Standard ML models\u00b6","text":""},{"location":"examples/OG_Tutorial/#4-understanding-og","title":"4. Understanding OG\u00b6","text":""},{"location":"examples/OG_Tutorial/#the-problem-local-overfitting","title":"The Problem: Local Overfitting\u00b6","text":"<p>When data is non-uniformly distributed:</p> <ul> <li>Dense areas: Models overfit, achieving high accuracy</li> <li>Sparse areas: Models underperform, poor generalization</li> </ul>"},{"location":"examples/OG_Tutorial/#the-solution-og-framework","title":"The Solution: OG Framework\u00b6","text":"<p>Stage 1 (Overfit): High-variance model (e.g., LightGBM) deliberately overfits to capture local patterns.</p> <p>Stage 2 (Generalize): Low-variance model (e.g., MLP) learns from pseudo-labels with:</p> <ul> <li>Density-aware sampling: Prioritize sparse regions</li> <li>Oscillation noise: Regularization through noise injection</li> </ul>"},{"location":"examples/OG_Tutorial/#key-parameters","title":"Key Parameters\u00b6","text":"Parameter Description Effect <code>oscillation</code> Noise level (0.01-0.1) Higher = more regularization <code>sampling_alpha</code> Density weight (0-1) Higher = prioritize sparse areas <code>epochs</code> Training iterations More = better fit, risk of overfit"},{"location":"examples/OG_Tutorial/#available-presets","title":"Available Presets\u00b6","text":"HV Models LV Models <code>'lightgbm'</code> <code>'mlp'</code> <code>'xgboost'</code> <code>'resnet'</code> <code>'catboost'</code> <code>'transformer'</code>"},{"location":"examples/tutorial/","title":"Tutorial","text":"<p>This tutorial walks through a complete OG-Learn workflow.</p>"},{"location":"examples/tutorial/#setup","title":"Setup","text":"<pre><code>import numpy as np\nimport pandas as pd\nimport warnings\nimport shutil\nimport os\n\n# Suppress warnings\nwarnings.filterwarnings('ignore')\n\n# Import og_learn\nimport og_learn\nfrom og_learn import (\n    OGModel, \n    compare_models,\n    calculate_density,\n    split_test_train,\n    load_data,\n    sanity_check,\n    save_split_indices,\n    load_split_indices,\n    launch_tensorboard,\n    list_presets\n)\nfrom og_learn.feature import simple_feature_engineering\nfrom og_learn.presets import get_hv_model, get_lv_model\n</code></pre>"},{"location":"examples/tutorial/#1-data-preparation","title":"1. Data Preparation","text":""},{"location":"examples/tutorial/#load-data","title":"Load Data","text":"<pre><code># Configuration\nDATA_PATH = '../data/your_data.parquet'\nSAVE_DIR = '../diagnostics'\nSAMPLE = 10000  # Use subset for quick testing\n\n# Define features\nFEATURE_COLS = [\n    'longitude', 'latitude',\n    'temperature', 'humidity', 'pressure'\n]\nTARGET_COL = 'ozone'\n\n# Load data\ndf = load_data(DATA_PATH, FEATURE_COLS, TARGET_COL)\n\n# Sample for testing\nif SAMPLE:\n    df = df.sample(n=min(SAMPLE, len(df)), random_state=42)\n\nprint(f\"Data shape: {df.shape}\")\n</code></pre>"},{"location":"examples/tutorial/#validate-data","title":"Validate Data","text":"<pre><code>sanity_check(df, FEATURE_COLS, TARGET_COL)\n</code></pre>"},{"location":"examples/tutorial/#feature-engineering","title":"Feature Engineering","text":"<pre><code>df, FEATURE_COLS = simple_feature_engineering(\n    df, FEATURE_COLS,\n    time_col='time',\n    k=3,\n    standardize=True,\n    add_temporal=True\n)\n\nprint(f\"Features after engineering: {len(FEATURE_COLS)}\")\n</code></pre>"},{"location":"examples/tutorial/#calculate-density","title":"Calculate Density","text":"<pre><code>density = calculate_density(df['longitude'], df['latitude'])\ndf['density'] = density\n</code></pre>"},{"location":"examples/tutorial/#split-data","title":"Split Data","text":"<pre><code># Site-wise split to prevent data leakage\ntrain_idx, test_idx = split_test_train(df, method='site', test_ratio=0.2)\n\n# Save for reproducibility\nsave_split_indices(train_idx, test_idx, SAVE_DIR)\n\n# Prepare arrays\nX_train = df.loc[train_idx, FEATURE_COLS].values\ny_train = df.loc[train_idx, TARGET_COL].values\nX_test = df.loc[test_idx, FEATURE_COLS].values\ny_test = df.loc[test_idx, TARGET_COL].values\ndensity_train = df.loc[train_idx, 'density'].values\n\nprint(f\"Train: {len(train_idx)}, Test: {len(test_idx)}\")\n</code></pre>"},{"location":"examples/tutorial/#2-model-training","title":"2. Model Training","text":""},{"location":"examples/tutorial/#single-og-model","title":"Single OG Model","text":"<pre><code>model = OGModel(\n    hv='lightgbm',\n    lv='mlp',\n    oscillation=0.05,\n    sampling_alpha=0.1,\n    epochs=100,\n    seed=42\n)\n\nmodel.fit(X_train, y_train, density=density_train)\n\npredictions = model.predict(X_test)\nprint(f\"Test R\u00b2: {r2_score(y_test, predictions):.4f}\")\n</code></pre>"},{"location":"examples/tutorial/#compare-multiple-models","title":"Compare Multiple Models","text":"<pre><code># Clear old TensorBoard logs\nTB_LOG_DIR = os.path.join(SAVE_DIR, 'tensorboard')\nshutil.rmtree(TB_LOG_DIR, ignore_errors=True)\n\n# Define models\nmodels = {\n    'MLP': get_lv_model('mlp', num_features=X_train.shape[1]),\n    'BigMLP': get_lv_model('bigmlp', num_features=X_train.shape[1]),\n    'OG_LightGBM_MLP': OGModel(hv='lightgbm', lv='mlp'),\n    'OG_XGBoost_MLP': OGModel(hv='xgboost', lv='mlp'),\n    'OG_CatBoost_ResNet': OGModel(hv='catboost', lv='resnet'),\n}\n\n# Run comparison\nresults = compare_models(\n    models,\n    X_train, y_train,\n    X_test, y_test,\n    density=density_train,\n    tensorboard_dir=TB_LOG_DIR,\n    save_dir=os.path.join(SAVE_DIR, 'models'),\n    eval_every_epochs=5\n)\n\nprint(results)\n</code></pre>"},{"location":"examples/tutorial/#3-tensorboard-visualization","title":"3. TensorBoard Visualization","text":"<pre><code># Launch TensorBoard\ntb_process = launch_tensorboard(TB_LOG_DIR, open_browser=True)\n\n# View at http://localhost:6006\n\n# When done:\n# tb_process.terminate()\n</code></pre>"},{"location":"examples/tutorial/#4-results-analysis","title":"4. Results Analysis","text":"<pre><code>import matplotlib.pyplot as plt\n\n# Plot comparison\nfig, ax = plt.subplots(figsize=(10, 6))\nresults.plot(kind='bar', x='Model', y=['Train_R2', 'Test_R2'], ax=ax)\nax.set_ylabel('R\u00b2 Score')\nax.set_title('Model Comparison')\nax.legend(['Train', 'Test'])\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"examples/tutorial/#next-steps","title":"Next Steps","text":"<ul> <li>Try different HV/LV combinations</li> <li>Tune <code>oscillation</code> and <code>sampling_alpha</code></li> <li>Use <code>early_stopping</code> with validation data</li> <li>Explore Custom Models</li> </ul>"},{"location":"getting-started/installation/","title":"Installation","text":""},{"location":"getting-started/installation/#requirements","title":"Requirements","text":"<ul> <li>Python 3.8 or higher</li> <li>PyTorch 1.9 or higher</li> <li>NumPy, Pandas, Scikit-learn</li> </ul>"},{"location":"getting-started/installation/#install-via-pip","title":"Install via pip","text":"<pre><code>pip install og-learn\n</code></pre>"},{"location":"getting-started/installation/#install-from-source","title":"Install from Source","text":"<p>For the latest development version:</p> <pre><code>git clone https://github.com/your-username/og-learn.git\ncd og-learn\npip install -e .\n</code></pre>"},{"location":"getting-started/installation/#dependencies","title":"Dependencies","text":"<p>OG-Learn will automatically install the following dependencies:</p> Package Purpose <code>torch</code> Neural network models (MLP, ResNet, Transformer) <code>numpy</code> Numerical operations <code>pandas</code> Data manipulation <code>scikit-learn</code> Preprocessing and metrics <code>lightgbm</code> LightGBM HV model preset <code>xgboost</code> XGBoost HV model preset <code>catboost</code> CatBoost HV model preset <code>tqdm</code> Progress bars <code>tensorboard</code> Training visualization"},{"location":"getting-started/installation/#optional-dependencies","title":"Optional Dependencies","text":"<p>For full functionality, you may also want to install:</p> <pre><code># For TensorBoard visualization\npip install tensorboard\n\n# For Jupyter notebook support\npip install jupyter ipywidgets\n</code></pre>"},{"location":"getting-started/installation/#verify-installation","title":"Verify Installation","text":"<pre><code>import og_learn\nprint(og_learn.__version__)\n\n# Check available presets\nfrom og_learn import list_presets\nlist_presets()\n</code></pre> <p>Expected output:</p> <pre><code>Available HV Presets:\n  - lightgbm\n  - biglightgbm\n  - xgboost\n  - catboost\n  - random_forest\n  - decision_tree\n  - linear_regression\n\nAvailable LV Presets:\n  - mlp\n  - bigmlp\n  - resnet\n  - transformer\n</code></pre>"},{"location":"getting-started/quickstart/","title":"Quick Start","text":"<p>This guide will walk you through the basics of using OG-Learn.</p>"},{"location":"getting-started/quickstart/#basic-usage","title":"Basic Usage","text":""},{"location":"getting-started/quickstart/#1-import-and-create-model","title":"1. Import and Create Model","text":"<pre><code>from og_learn import OGModel\n\n# Create OG model with presets\nmodel = OGModel(\n    hv='lightgbm',      # High-variance model\n    lv='mlp',           # Low-variance model\n    oscillation=0.05,   # Feature noise regularization\n    sampling_alpha=0.1  # Density-aware sampling weight\n)\n</code></pre>"},{"location":"getting-started/quickstart/#2-prepare-your-data","title":"2. Prepare Your Data","text":"<p>OG-Learn works with standard NumPy arrays or Pandas DataFrames:</p> <pre><code>import numpy as np\nfrom og_learn import calculate_density\n\n# Your data\nX_train = ...  # Features (n_samples, n_features)\ny_train = ...  # Target (n_samples,)\n\n# Calculate spatial density (required for OG)\n# Assumes columns 0, 1 are longitude, latitude\ndensity_train = calculate_density(X_train[:, 0], X_train[:, 1])\n</code></pre>"},{"location":"getting-started/quickstart/#3-train-the-model","title":"3. Train the Model","text":"<pre><code>model.fit(\n    X_train, y_train,\n    density=density_train,\n    epochs=100,\n    X_valid=X_valid,  # Optional: for early stopping\n    y_valid=y_valid\n)\n</code></pre>"},{"location":"getting-started/quickstart/#4-make-predictions","title":"4. Make Predictions","text":"<pre><code>predictions = model.predict(X_test)\n</code></pre>"},{"location":"getting-started/quickstart/#5-evaluate","title":"5. Evaluate","text":"<pre><code>from sklearn.metrics import r2_score, mean_squared_error\n\nr2 = r2_score(y_test, predictions)\nrmse = np.sqrt(mean_squared_error(y_test, predictions))\n\nprint(f\"R\u00b2: {r2:.4f}\")\nprint(f\"RMSE: {rmse:.4f}\")\n</code></pre>"},{"location":"getting-started/quickstart/#complete-example","title":"Complete Example","text":"<pre><code>import numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score\nfrom og_learn import OGModel, calculate_density\n\n# Generate sample data\nnp.random.seed(42)\nn_samples = 1000\nlon = np.random.uniform(-180, 180, n_samples)\nlat = np.random.uniform(-90, 90, n_samples)\nX = np.column_stack([lon, lat, np.random.randn(n_samples, 5)])\ny = np.sin(lon/30) + np.cos(lat/20) + 0.1 * np.random.randn(n_samples)\n\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n\n# Calculate density\ndensity_train = calculate_density(X_train[:, 0], X_train[:, 1])\n\n# Create and train OG model\nmodel = OGModel(\n    hv='lightgbm',\n    lv='mlp',\n    oscillation=0.05,\n    sampling_alpha=0.1,\n    seed=42\n)\n\nmodel.fit(X_train, y_train, density=density_train, epochs=50)\n\n# Evaluate\npredictions = model.predict(X_test)\nprint(f\"Test R\u00b2: {r2_score(y_test, predictions):.4f}\")\n</code></pre>"},{"location":"getting-started/quickstart/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about the OG Framework in detail</li> <li>Explore Preset Models</li> <li>Create Custom Models</li> <li>Compare models with Model Comparison</li> </ul>"},{"location":"guide/custom-models/","title":"Custom Models","text":"<p>OG-Learn allows you to use any model that follows the scikit-learn interface.</p>"},{"location":"guide/custom-models/#custom-hv-model","title":"Custom HV Model","text":"<p>Any model with <code>fit(X, y)</code> and <code>predict(X)</code> methods can be used as an HV model:</p> <pre><code>from og_learn import OGModel\nfrom sklearn.ensemble import GradientBoostingRegressor\n\n# Create custom HV model\ncustom_hv = GradientBoostingRegressor(\n    n_estimators=200,\n    max_depth=5,\n    learning_rate=0.1\n)\n\n# Use in OGModel\nmodel = OGModel(\n    hv=custom_hv,  # Pass model instance\n    lv='mlp',\n    oscillation=0.05,\n    sampling_alpha=0.1\n)\n\nmodel.fit(X_train, y_train, density=density_train)\n</code></pre>"},{"location":"guide/custom-models/#custom-lv-model","title":"Custom LV Model","text":"<p>For custom LV models, you need to implement a class with <code>fit()</code> and <code>predict()</code> methods:</p> <pre><code>import torch\nimport torch.nn as nn\nfrom og_learn import OGModel\n\nclass CustomMLP:\n    def __init__(self, input_dim, hidden_dims=[128, 64]):\n        self.input_dim = input_dim\n        self.hidden_dims = hidden_dims\n        self.model = None\n\n    def _build_model(self):\n        layers = []\n        prev_dim = self.input_dim\n        for dim in self.hidden_dims:\n            layers.extend([\n                nn.Linear(prev_dim, dim),\n                nn.ReLU(),\n                nn.Dropout(0.2)\n            ])\n            prev_dim = dim\n        layers.append(nn.Linear(prev_dim, 1))\n        return nn.Sequential(*layers)\n\n    def fit(self, X, y, epochs=100, **kwargs):\n        self.model = self._build_model()\n        # Training logic here...\n        return self\n\n    def predict(self, X):\n        self.model.eval()\n        with torch.no_grad():\n            X_tensor = torch.FloatTensor(X)\n            return self.model(X_tensor).numpy().flatten()\n\n# Use custom LV model\ncustom_lv = CustomMLP(input_dim=X_train.shape[1])\nmodel = OGModel(\n    hv='lightgbm',\n    lv=custom_lv,  # Pass model instance\n    oscillation=0.05\n)\n</code></pre>"},{"location":"guide/custom-models/#using-og-core-functions-directly","title":"Using OG Core Functions Directly","text":"<p>For maximum flexibility, use the core OG functions:</p> <pre><code>from og_learn.og_core import (\n    initialize_OG_componment,\n    generate_OG_componment\n)\n\n# Stage 1: Initialize and fit HV model\nhv_model = initialize_OG_componment(X_train, y_train, model_type='lightgbm')\n\n# Generate pseudo-labels with density-aware sampling\nX_pseudo, y_pseudo = generate_OG_componment(\n    X_train, y_train,\n    density=density_train,\n    hv_model=hv_model,\n    oscillation=0.05,\n    sampling_alpha=0.1\n)\n\n# Stage 2: Train your own LV model on pseudo-labels\nfrom sklearn.neural_network import MLPRegressor\n\nlv_model = MLPRegressor(hidden_layer_sizes=(256, 128, 64))\nlv_model.fit(X_pseudo, y_pseudo)\n\n# Predict\npredictions = lv_model.predict(X_test)\n</code></pre>"},{"location":"guide/custom-models/#hybrid-approaches","title":"Hybrid Approaches","text":"<p>Combine preset and custom models:</p> <pre><code>from og_learn import OGModel\nfrom og_learn.presets import get_hv_model\nfrom catboost import CatBoostRegressor\n\n# Custom CatBoost with specific parameters\ncustom_catboost = CatBoostRegressor(\n    iterations=1000,\n    depth=10,\n    learning_rate=0.03,\n    l2_leaf_reg=5,\n    verbose=False\n)\n\n# Use with preset LV model\nmodel = OGModel(\n    hv=custom_catboost,\n    lv='resnet',\n    oscillation=0.03,\n    sampling_alpha=0.15\n)\n\nmodel.fit(X_train, y_train, density=density_train, epochs=150)\n</code></pre>"},{"location":"guide/custom-models/#tips-for-custom-models","title":"Tips for Custom Models","text":"<p>HV Model Requirements</p> <ul> <li>Must have <code>fit(X, y)</code> method</li> <li>Must have <code>predict(X)</code> method</li> <li>Should be capable of learning local patterns (tree-based models work well)</li> </ul> <p>LV Model Requirements</p> <ul> <li>Must have <code>fit(X, y, ...)</code> method</li> <li>Must have <code>predict(X)</code> method</li> <li>For full OG integration, implement <code>OG_componment</code> support in <code>fit()</code></li> </ul> <p>Avoid</p> <ul> <li>Linear models as HV (won't capture local patterns)</li> <li>Very complex models as LV (defeats the purpose of generalization)</li> </ul>"},{"location":"guide/feature-engineering/","title":"Feature Engineering","text":"<p>OG-Learn provides utilities for spatiotemporal feature engineering.</p>"},{"location":"guide/feature-engineering/#overview","title":"Overview","text":"<p>Effective feature engineering is crucial for spatiotemporal models. OG-Learn includes functions for:</p> <ul> <li>Loading and preprocessing data</li> <li>Computing spatial harmonics</li> <li>Creating temporal features</li> <li>Feature standardization</li> </ul>"},{"location":"guide/feature-engineering/#loading-data","title":"Loading Data","text":"<pre><code>from og_learn import load_data\n\n# Load and preprocess\ndf = load_data(\n    data_path='data/observations.parquet',\n    feature_cols=['longitude', 'latitude', 'temp', 'humidity'],\n    target_col='ozone'\n)\n</code></pre> <p>The <code>load_data</code> function:</p> <ul> <li>Converts <code>time</code> column to datetime</li> <li>Converts float64 to float32 for memory efficiency</li> <li>Filters to valid feature columns</li> </ul>"},{"location":"guide/feature-engineering/#spatial-harmonics","title":"Spatial Harmonics","text":"<p>Transform longitude/latitude into harmonic features:</p> <pre><code>from og_learn.feature import compute_spatial_harmonics\n\n# Add spatial harmonics\ndf = compute_spatial_harmonics(\n    df,\n    k=3,  # Number of harmonics\n    lon_col='longitude',\n    lat_col='latitude'\n)\n</code></pre> <p>This creates features:</p> <ul> <li><code>lon_sin_1</code>, <code>lon_cos_1</code>, <code>lat_sin_1</code>, <code>lat_cos_1</code></li> <li><code>lon_sin_2</code>, <code>lon_cos_2</code>, <code>lat_sin_2</code>, <code>lat_cos_2</code></li> <li><code>lon_sin_3</code>, <code>lon_cos_3</code>, <code>lat_sin_3</code>, <code>lat_cos_3</code></li> </ul> <p>The transformation is:</p> \\[\\sin(k \\cdot \\frac{\\text{lon}}{180} \\cdot \\pi), \\quad \\cos(k \\cdot \\frac{\\text{lon}}{180} \\cdot \\pi)\\]"},{"location":"guide/feature-engineering/#temporal-features","title":"Temporal Features","text":"<p>Extract time-based features:</p> <pre><code>from og_learn.feature import compute_temporal_features\n\n# Add temporal features\ndf = compute_temporal_features(df, time_col='time')\n</code></pre> <p>Created features:</p> Feature Description <code>time_month</code> Month (1-12), normalized <code>time_hour</code> Hour (0-23), normalized <code>time_day_of_month</code> Day (1-31), normalized"},{"location":"guide/feature-engineering/#complete-feature-engineering-pipeline","title":"Complete Feature Engineering Pipeline","text":"<pre><code>from og_learn.feature import simple_feature_engineering\n\n# Apply full pipeline\ndf_processed, feature_cols = simple_feature_engineering(\n    df,\n    feature_cols=['longitude', 'latitude', 'temp', 'humidity', 'pressure'],\n    time_col='time',\n    k=3,                  # Spatial harmonics\n    standardize=True,     # Apply StandardScaler\n    add_temporal=True     # Add temporal features\n)\n\nprint(f\"Original features: 5\")\nprint(f\"Processed features: {len(feature_cols)}\")\n</code></pre> <p>Output:</p> <pre><code>Original features: 5\nProcessed features: 20\n</code></pre>"},{"location":"guide/feature-engineering/#sanity-check","title":"Sanity Check","text":"<p>Validate your data before training:</p> <pre><code>from og_learn import sanity_check\n\nsanity_check(df, feature_cols, target_col='ozone')\n</code></pre> <p>Output:</p> <pre><code>============================================================\n              Data Sanity Check\n============================================================\nTotal samples: 50,000\nFeatures: 10\nMissing values: 0\n\nFeature Engineering Options:\n  \u2022 Spatial harmonics: k=3 (adds 12 features)\n  \u2022 Temporal features: month, hour, day (adds 3 features)\n  \u2022 Standardization: recommended\n\nData looks good! \u2713\n============================================================\n</code></pre>"},{"location":"guide/feature-engineering/#best-practices","title":"Best Practices","text":"<p>Spatial Harmonics</p> <ul> <li>Use <code>k=3</code> for most cases</li> <li>Higher <code>k</code> captures finer spatial patterns but may overfit</li> </ul> <p>Standardization</p> <ul> <li>Always standardize features for neural network LV models</li> <li>Apply standardization before spatial harmonics</li> </ul> <p>Data Leakage</p> <ul> <li>Fit StandardScaler on training data only</li> <li>Use <code>split_test_train</code> before feature engineering</li> </ul>"},{"location":"guide/og-framework/","title":"The OG Framework","text":""},{"location":"guide/og-framework/#overview","title":"Overview","text":"<p>The Overfit-to-Generalization (OG) framework is a two-stage machine learning approach designed to improve prediction quality in spatiotemporal tasks, particularly when dealing with heterogeneous data density.</p>"},{"location":"guide/og-framework/#motivation","title":"Motivation","text":"<p>Traditional machine learning models face challenges with spatiotemporal data:</p> Challenge Description Density Imbalance Some regions have dense observations while others are sparse Local Overfitting Models may memorize patterns in dense areas without learning generalizable features Spatial Heterogeneity Different spatial regions may have fundamentally different relationships"},{"location":"guide/og-framework/#two-stage-architecture","title":"Two-Stage Architecture","text":""},{"location":"guide/og-framework/#stage-1-high-variance-hv-model","title":"Stage 1: High-Variance (HV) Model","text":"<p>The HV model is designed to:</p> <ol> <li>Capture local patterns through high model complexity</li> <li>Generate pseudo-labels for training the LV model</li> <li>Apply density-aware sampling to balance learning across regions</li> </ol> <pre><code># HV model options\nhv_models = ['lightgbm', 'xgboost', 'catboost', 'random_forest']\n</code></pre>"},{"location":"guide/og-framework/#oscillation-noise","title":"Oscillation Noise","text":"<p>During pseudo-label generation, oscillation noise is injected into features:</p> \\[X_{noisy} = X + \\epsilon \\cdot \\sigma_X \\cdot \\mathcal{N}(0, 1)\\] <p>where \\(\\epsilon\\) is the oscillation parameter (typically 0.05) and \\(\\sigma_X\\) is the feature standard deviation.</p> <p>This regularization technique helps the LV model learn smoother, more generalizable patterns.</p>"},{"location":"guide/og-framework/#stage-2-low-variance-lv-model","title":"Stage 2: Low-Variance (LV) Model","text":"<p>The LV model is trained to:</p> <ol> <li>Learn from pseudo-labels generated by the HV model</li> <li>Capture generalizable patterns through controlled model capacity</li> <li>Apply density-aware weighting during training</li> </ol> <pre><code># LV model options\nlv_models = ['mlp', 'bigmlp', 'resnet', 'transformer']\n</code></pre>"},{"location":"guide/og-framework/#density-aware-sampling","title":"Density-Aware Sampling","text":"<p>A key innovation in OG is density-aware sampling, which prioritizes learning from sparse regions:</p> \\[w_i = \\frac{1}{density_i^\\alpha}\\] <p>where: - \\(w_i\\) is the sampling weight for observation \\(i\\) - \\(density_i\\) is the local data density - \\(\\alpha\\) is the sampling parameter (typically 0.1)</p> <p>This ensures the model doesn't overfit to dense regions at the expense of sparse areas.</p>"},{"location":"guide/og-framework/#using-ogmodel","title":"Using OGModel","text":""},{"location":"guide/og-framework/#basic-usage","title":"Basic Usage","text":"<pre><code>from og_learn import OGModel\n\nmodel = OGModel(\n    hv='lightgbm',       # HV model type\n    lv='mlp',            # LV model type\n    oscillation=0.05,    # Noise injection strength\n    sampling_alpha=0.1,  # Density sampling exponent\n    epochs=100,          # Training epochs for LV\n    seed=42              # Random seed for reproducibility\n)\n\nmodel.fit(X_train, y_train, density=density_train)\npredictions = model.predict(X_test)\n</code></pre>"},{"location":"guide/og-framework/#with-validation-and-early-stopping","title":"With Validation and Early Stopping","text":"<pre><code>model = OGModel(\n    hv='lightgbm',\n    lv='resnet',\n    oscillation=0.05,\n    sampling_alpha=0.1,\n    epochs=200,\n    early_stopping=True,\n    patience=20\n)\n\nmodel.fit(\n    X_train, y_train,\n    density=density_train,\n    X_valid=X_valid,\n    y_valid=y_valid\n)\n</code></pre>"},{"location":"guide/og-framework/#with-tensorboard-logging","title":"With TensorBoard Logging","text":"<pre><code>model = OGModel(\n    hv='lightgbm',\n    lv='mlp',\n    tensorboard_dir='runs/experiment1',\n    tensorboard_name='og_mlp',\n    eval_every_epochs=5\n)\n\nmodel.fit(X_train, y_train, density=density_train)\n</code></pre>"},{"location":"guide/og-framework/#model-comparison","title":"Model Comparison","text":"<p>Use <code>compare_models</code> to benchmark different configurations:</p> <pre><code>from og_learn import OGModel, compare_models\nfrom og_learn.presets import get_lv_model\n\n# Define models to compare\nmodels = {\n    'MLP': get_lv_model('mlp', num_features=X_train.shape[1]),\n    'OG_MLP': OGModel(hv='lightgbm', lv='mlp'),\n    'OG_ResNet': OGModel(hv='lightgbm', lv='resnet'),\n}\n\n# Run comparison\nresults = compare_models(\n    models,\n    X_train, y_train,\n    X_test, y_test,\n    density=density_train,\n    tensorboard_dir='runs/comparison',\n    save_dir='checkpoints'\n)\n\n# View results\nprint(results)\n</code></pre> <p>Output:</p> <pre><code>============================================================\n         Model Comparison\n============================================================\n              Model  Train_R2  Test_R2\n0               MLP    0.6012   0.5743\n1            OG_MLP    0.6524   0.6127\n2          OG_ResNet   0.6891   0.6352\n============================================================\n</code></pre>"},{"location":"guide/og-framework/#best-practices","title":"Best Practices","text":"<p>Choosing HV Models</p> <ul> <li>LightGBM: Fast, good default choice</li> <li>XGBoost: Often slightly better accuracy</li> <li>CatBoost: Better with categorical features</li> </ul> <p>Choosing LV Models</p> <ul> <li>MLP: Simple, fast, good baseline</li> <li>ResNet: Better for complex patterns</li> <li>Transformer: Best for sequential/temporal patterns</li> </ul> <p>Common Pitfalls</p> <ul> <li>Don't forget to provide <code>density</code> during training</li> <li>Use validation data for early stopping with neural networks</li> <li>Set <code>seed</code> for reproducibility</li> </ul>"},{"location":"guide/presets/","title":"Preset Models","text":"<p>OG-Learn provides carefully tuned preset configurations for both HV and LV models.</p>"},{"location":"guide/presets/#high-variance-hv-presets","title":"High-Variance (HV) Presets","text":"<p>These models are used in Stage 1 for pseudo-label generation.</p>"},{"location":"guide/presets/#lightgbm-presets","title":"LightGBM Presets","text":"lightgbm (default)biglightgbm <p>Standard LightGBM configuration for most use cases.</p> <pre><code>OGModel(hv='lightgbm', lv='mlp')\n</code></pre> Parameter Value n_estimators 500 num_leaves 1200 max_depth 9 learning_rate 0.05 <p>Larger LightGBM for complex datasets.</p> <pre><code>OGModel(hv='biglightgbm', lv='mlp')\n</code></pre> Parameter Value n_estimators 500 num_leaves 1200 max_depth 9 min_child_samples 1"},{"location":"guide/presets/#xgboost","title":"XGBoost","text":"<pre><code>OGModel(hv='xgboost', lv='mlp')\n</code></pre> Parameter Value n_estimators 500 max_depth 11 learning_rate 0.05"},{"location":"guide/presets/#catboost","title":"CatBoost","text":"<pre><code>OGModel(hv='catboost', lv='mlp')\n</code></pre> Parameter Value iterations 500 depth 9 learning_rate 0.05"},{"location":"guide/presets/#other-hv-models","title":"Other HV Models","text":"<pre><code># Random Forest\nOGModel(hv='random_forest', lv='mlp')\n\n# Decision Tree\nOGModel(hv='decision_tree', lv='mlp')\n\n# Linear Regression (baseline)\nOGModel(hv='linear_regression', lv='mlp')\n</code></pre>"},{"location":"guide/presets/#low-variance-lv-presets","title":"Low-Variance (LV) Presets","text":"<p>These neural network models are used in Stage 2 for learning from pseudo-labels.</p>"},{"location":"guide/presets/#mlp-multi-layer-perceptron","title":"MLP (Multi-Layer Perceptron)","text":"mlp (default)bigmlp <p>Standard MLP architecture.</p> <pre><code>OGModel(hv='lightgbm', lv='mlp')\n</code></pre> Parameter Value hidden_layers [256, 128, 64] dropout 0.3 batch_size 256 learning_rate 0.001 <p>Larger MLP for complex patterns.</p> <pre><code>OGModel(hv='lightgbm', lv='bigmlp')\n</code></pre> Parameter Value hidden_layers [512, 256, 128] dropout 0.3 batch_size 256 learning_rate 0.001"},{"location":"guide/presets/#resnet","title":"ResNet","text":"<p>Deep residual network with skip connections.</p> <pre><code>OGModel(hv='lightgbm', lv='resnet')\n</code></pre> Parameter Value d_main 256 d_hidden 512 n_blocks 2 dropout 0.2"},{"location":"guide/presets/#transformer","title":"Transformer","text":"<p>Attention-based architecture for capturing complex dependencies.</p> <pre><code>OGModel(hv='lightgbm', lv='transformer')\n</code></pre> Parameter Value n_layers 3 d_token 192 n_heads 8 d_ffn 256"},{"location":"guide/presets/#listing-available-presets","title":"Listing Available Presets","text":"<pre><code>from og_learn import list_presets\n\nlist_presets()\n</code></pre> <p>Output:</p> <pre><code>============================================================\n              Available Presets\n============================================================\n\nHV (High-Variance) Presets:\n  \u2022 lightgbm\n  \u2022 biglightgbm\n  \u2022 xgboost\n  \u2022 catboost\n  \u2022 random_forest\n  \u2022 decision_tree\n  \u2022 linear_regression\n\nLV (Low-Variance) Presets:\n  \u2022 mlp\n  \u2022 bigmlp\n  \u2022 resnet\n  \u2022 transformer\n\n============================================================\n</code></pre>"},{"location":"guide/presets/#getting-preset-models-directly","title":"Getting Preset Models Directly","text":"<p>You can also get preset models without using OGModel:</p> <pre><code>from og_learn.presets import get_hv_model, get_lv_model\n\n# Get HV model\nhv_model = get_hv_model('lightgbm')\n\n# Get LV model (requires num_features)\nlv_model = get_lv_model('mlp', num_features=10, epochs=100)\n\n# Use directly\nhv_model.fit(X_train, y_train)\npredictions = hv_model.predict(X_test)\n</code></pre>"},{"location":"guide/tensorboard/","title":"TensorBoard Integration","text":"<p>OG-Learn provides built-in TensorBoard support for visualizing training progress.</p>"},{"location":"guide/tensorboard/#basic-setup","title":"Basic Setup","text":""},{"location":"guide/tensorboard/#enable-tensorboard-logging","title":"Enable TensorBoard Logging","text":"<pre><code>from og_learn import OGModel\n\nmodel = OGModel(\n    hv='lightgbm',\n    lv='mlp',\n    tensorboard_dir='runs/experiment1',  # Log directory\n    tensorboard_name='og_mlp',           # Run name\n    eval_every_epochs=5                   # Log frequency\n)\n\nmodel.fit(\n    X_train, y_train,\n    density=density_train,\n    X_valid=X_valid,  # Required for validation metrics\n    y_valid=y_valid,\n    epochs=100\n)\n</code></pre>"},{"location":"guide/tensorboard/#launch-tensorboard","title":"Launch TensorBoard","text":"<pre><code>from og_learn import launch_tensorboard\n\n# Start TensorBoard server\ntb_process = launch_tensorboard('runs/experiment1', open_browser=True)\n</code></pre> <p>Or from command line:</p> <pre><code>tensorboard --logdir=runs/experiment1\n</code></pre> <p>Then open http://localhost:6006 in your browser.</p>"},{"location":"guide/tensorboard/#comparing-multiple-models","title":"Comparing Multiple Models","text":"<pre><code>from og_learn import OGModel, compare_models\nfrom og_learn.presets import get_lv_model\n\n# Clear old logs\nimport shutil\nshutil.rmtree('runs/comparison', ignore_errors=True)\n\n# Define models\nmodels = {\n    'MLP': get_lv_model('mlp', num_features=X_train.shape[1]),\n    'OG_LightGBM_MLP': OGModel(hv='lightgbm', lv='mlp'),\n    'OG_XGBoost_MLP': OGModel(hv='xgboost', lv='mlp'),\n    'OG_CatBoost_ResNet': OGModel(hv='catboost', lv='resnet'),\n}\n\n# Run comparison with TensorBoard logging\nresults = compare_models(\n    models,\n    X_train, y_train,\n    X_test, y_test,\n    density=density_train,\n    tensorboard_dir='runs/comparison',  # All models log here\n    eval_every_epochs=5\n)\n\n# Launch TensorBoard to compare\nlaunch_tensorboard('runs/comparison', open_browser=True)\n</code></pre>"},{"location":"guide/tensorboard/#what-gets-logged","title":"What Gets Logged","text":"Metric Description <code>R2_train</code> Training R\u00b2 score <code>R2_val</code> Validation R\u00b2 score <code>Loss_train</code> Training loss (MSE)"},{"location":"guide/tensorboard/#viewing-in-tensorboard","title":"Viewing in TensorBoard","text":""},{"location":"guide/tensorboard/#scalars-tab","title":"Scalars Tab","text":"<p>View training curves:</p> <ul> <li>R\u00b2 scores over epochs</li> <li>Loss over epochs</li> <li>Compare multiple runs side-by-side</li> </ul>"},{"location":"guide/tensorboard/#text-tab","title":"Text Tab","text":"<p>View run configuration and parameters.</p>"},{"location":"guide/tensorboard/#tips","title":"Tips","text":"<p>Clear Old Logs</p> <p>Before running new experiments, clear old logs to avoid confusion: <pre><code>import shutil\nshutil.rmtree('runs/', ignore_errors=True)\n</code></pre></p> <p>Logging Frequency</p> <p>Set <code>eval_every_epochs=1</code> for detailed curves, or higher values (5-10) for faster training.</p> <p>Validation Data</p> <p>Always provide <code>X_valid</code> and <code>y_valid</code> to see validation metrics in TensorBoard.</p> <p>Background Process</p> <p><code>launch_tensorboard()</code> runs in background. To stop: <pre><code>tb_process.terminate()\n</code></pre></p>"}]}